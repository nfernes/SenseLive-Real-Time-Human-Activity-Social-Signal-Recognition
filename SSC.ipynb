{"cells":[{"cell_type":"markdown","id":"intro","metadata":{"id":"intro"},"source":["#Social Signal Classification with Hybrid CNN-RNN Models\n","\n","This notebook implements hybrid architectures combining CNNs for feature extraction with RNNs for temporal modeling:\n","1. **CNN-LSTM Model** - Captures long-term temporal dependencies\n","2. **CNN-GRU Model** - More efficient alternative to LSTM\n","3. **CNN-Bidirectional LSTM** - Processes sequences in both directions\n","4. **Attention-based CNN-LSTM** - Focuses on important time steps\n","\n","## Why Hybrid Models Work Better for Time-Series\n","- **CNN layers**: Extract local patterns and features from accelerometer data\n","- **RNN layers**: Model temporal dependencies and sequence patterns\n","- **Together**: Combine spatial feature extraction with temporal modeling"]},{"cell_type":"code","execution_count":null,"id":"imports","metadata":{"id":"imports"},"outputs":[],"source":["import os\n","import glob\n","import json\n","import zipfile\n","import shutil\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import (\n","    classification_report,\n","    confusion_matrix,\n","    accuracy_score,\n","    f1_score,\n","    precision_score,\n","    recall_score\n",")\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import (\n","    Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D,\n","    LSTM, GRU, Bidirectional, TimeDistributed,\n","    Dense, Dropout, BatchNormalization, Flatten, Input, Concatenate,\n","    Attention, MultiHeadAttention, LayerNormalization\n",")\n","from tensorflow.keras.callbacks import (\n","    EarlyStopping,\n","    ReduceLROnPlateau,\n","    ModelCheckpoint\n",")\n","\n","\n","print(f\"TensorFlow version: {tf.__version__}\")\n","print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"]},{"cell_type":"code","execution_count":null,"id":"config","metadata":{"id":"config"},"outputs":[],"source":["#configuration\n","notebook_dir = os.getcwd()\n","your_dataset_path = os.path.join(notebook_dir, \"data\", \"social_signal\")\n","\n","window_size = 50\n","step_size = 25\n","epochs = 50\n","batch_size = 32\n","\n","STATIC_THRESHOLD = 0.05\n","\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","MODEL_TYPE = 'cnn_gru'"]},{"cell_type":"markdown","id":"utility_functions","metadata":{"id":"utility_functions"},"source":["## Utility Functions"]},{"cell_type":"code","execution_count":null,"id":"utils_1","metadata":{"id":"utils_1"},"outputs":[],"source":["def calculate_acceleration_variance(data):\n","    accel_magnitude = np.sqrt(data[:, 0]**2 + data[:, 1]**2 + data[:, 2]**2)\n","    variance = np.var(accel_magnitude)\n","    is_static = variance < STATIC_THRESHOLD\n","    return variance, is_static\n","\n","def filter_static_windows(windows, labels):\n","    static_indices = []\n","    for i, window in enumerate(windows):\n","        _, is_static = calculate_acceleration_variance(window)\n","        if is_static:\n","            static_indices.append(i)\n","\n","\n","    if len(static_indices) == 0:\n","        return np.array([]), np.array([]), np.array([], dtype=int)\n","\n","    static_indices = np.array(static_indices, dtype=int)\n","    static_windows = windows[static_indices]\n","    static_labels = labels[static_indices]\n","\n","    return static_windows, static_labels, static_indices"]},{"cell_type":"code","execution_count":null,"id":"augmentation","metadata":{"id":"augmentation"},"outputs":[],"source":["def augment_window(window, aug_type='scale'):\n","\n","    if aug_type == 'scale':\n","        scale = np.random.uniform(0.90, 1.10)\n","        return window * scale\n","\n","    elif aug_type == 'noise':\n","        noise = np.random.normal(0, 0.01, window.shape)\n","        return window + noise\n","\n","    elif aug_type == 'shift':\n","        shift = np.random.randint(-3, 4)\n","        if shift > 0:\n","            return np.concatenate([window[shift:], np.repeat(window[-1:], shift, axis=0)], axis=0)\n","        elif shift < 0:\n","            return np.concatenate([np.repeat(window[:1], -shift, axis=0), window[:shift]], axis=0)\n","        return window\n","\n","    elif aug_type == 'magnitude':\n","        mag_scale = np.random.uniform(0.93, 1.07)\n","        magnitude = np.sqrt(np.sum(window**2, axis=1, keepdims=True))\n","        direction = window / (magnitude + 1e-8)\n","        new_magnitude = magnitude * mag_scale\n","        return direction * new_magnitude\n","\n","    elif aug_type == 'rotation':\n","\n","        angle = np.random.uniform(-0.1, 0.1)\n","        cos_a, sin_a = np.cos(angle), np.sin(angle)\n","        rotation_matrix = np.array([[cos_a, -sin_a, 0],\n","                                   [sin_a, cos_a, 0],\n","                                   [0, 0, 1]])\n","        return window @ rotation_matrix.T\n","\n","    return window"]},{"cell_type":"code","execution_count":null,"id":"data_loading","metadata":{"id":"data_loading"},"outputs":[],"source":["def load_files_from_folder(folder_path):\n","    file_paths = []\n","    for file_name in os.listdir(folder_path):\n","        if file_name.endswith('.csv'):\n","            full_file_path = os.path.join(folder_path, file_name)\n","            file_paths.append(full_file_path)\n","    return file_paths\n","\n","def split_files(file_list, test_size=0.2):\n","    if len(file_list) == 0:\n","        return [], []\n","    if len(file_list) == 1:\n","        print(f\"Only 1 file found\")\n","        return file_list, []\n","    if len(file_list) == 2:\n","        return [file_list[0]], [file_list[1]]\n","\n","    train_files, test_files = train_test_split(file_list, test_size=test_size,\n","                                                shuffle=True, random_state=42)\n","    return train_files, test_files\n","\n","def load_and_apply_sliding_windows(file_paths, window_size, step_size, label):\n","    windows = []\n","    labels = []\n","\n","    for file_path in file_paths:\n","        try:\n","            data = pd.read_csv(file_path, usecols=['accelX', 'accelY', 'accelZ'])\n","        except KeyError:\n","            try:\n","                data = pd.read_csv(file_path, usecols=['accel_x', 'accel_y', 'accel_z'])\n","            except KeyError:\n","                continue\n","\n","        data = data.to_numpy()\n","        num_samples = data.shape[0]\n","\n","        for i in range(0, num_samples - window_size + 1, step_size):\n","            window = data[i:i + window_size]\n","            windows.append(window)\n","            labels.append(label)\n","\n","            if label == 0:\n","                aug_prob = 0.6\n","                num_augmentations = 2\n","            elif label == 1:\n","                aug_prob = 0.9\n","                num_augmentations = 3\n","            elif label == 2:\n","                aug_prob = 0.95\n","                num_augmentations = 3\n","            elif label == 3:\n","                aug_prob = 0.4\n","                num_augmentations = 1\n","\n","            if np.random.random() < aug_prob:\n","                aug_types = ['scale', 'noise', 'shift', 'magnitude', 'rotation']\n","                for aug_idx in range(num_augmentations):\n","                    aug_type = aug_types[aug_idx % len(aug_types)]\n","                    augmented = augment_window(window, aug_type)\n","                    windows.append(augmented)\n","                    labels.append(label)\n","\n","    return np.array(windows), np.array(labels)\n","\n","def process_activity(activity, label, dataset_path, window_size=50, step_size=25, test_size=0.2):\n","    folder_path = os.path.join(dataset_path, activity)\n","\n","    if not os.path.exists(folder_path):\n","        return np.array([]), np.array([]), np.array([]), np.array([])\n","\n","    file_list = load_files_from_folder(folder_path)\n","\n","    if len(file_list) == 0:\n","        return np.array([]), np.array([]), np.array([]), np.array([])\n","\n","    train_files, test_files = split_files(file_list, test_size=test_size)\n","\n","    train_windows, train_labels = load_and_apply_sliding_windows(\n","        train_files, window_size, step_size, label)\n","\n","    test_windows, test_labels = load_and_apply_sliding_windows(\n","        test_files, window_size, step_size, label)\n","\n","    return train_windows, train_labels, test_windows, test_labels\n","\n","def combine_data(train_test_data, data_type):\n","    windows_list = [train_test_data[activity][f'{data_type}_windows']\n","                    for activity in train_test_data\n","                    if len(train_test_data[activity][f'{data_type}_windows']) > 0]\n","\n","    labels_list = [train_test_data[activity][f'{data_type}_labels']\n","                   for activity in train_test_data\n","                   if len(train_test_data[activity][f'{data_type}_labels']) > 0]\n","\n","    if len(windows_list) == 0:\n","        return np.array([]), np.array([])\n","\n","    concatenated_windows = np.concatenate(windows_list, axis=0)\n","    concatenated_labels = np.concatenate(labels_list, axis=0)\n","\n","    return concatenated_windows, concatenated_labels"]},{"cell_type":"markdown","id":"hybrid_models","metadata":{"id":"hybrid_models"},"source":["## Hybrid Model Architectures"]},{"cell_type":"code","execution_count":null,"id":"cnn_lstm_model","metadata":{"id":"cnn_lstm_model"},"outputs":[],"source":["def build_cnn_lstm_model(input_shape, num_classes):\n","\n","    model = Sequential([\n","\n","        Conv1D(filters=64, kernel_size=5, activation='relu', padding='same', input_shape=input_shape),\n","        BatchNormalization(),\n","        Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),\n","        BatchNormalization(),\n","        MaxPooling1D(pool_size=2),\n","        Dropout(0.2),\n","\n","        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n","        BatchNormalization(),\n","        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n","        BatchNormalization(),\n","        MaxPooling1D(pool_size=2),\n","        Dropout(0.2),\n","\n","        LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n","        LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n","\n","        Dense(128, activation='relu'),\n","        BatchNormalization(),\n","        Dropout(0.3),\n","\n","        Dense(64, activation='relu'),\n","        BatchNormalization(),\n","        Dropout(0.2),\n","\n","        Dense(num_classes, activation='softmax')\n","    ])\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n","        metrics=['accuracy']\n","    )\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"cnn_gru_model","metadata":{"id":"cnn_gru_model"},"outputs":[],"source":["def build_cnn_gru_model(input_shape, num_classes):\n","\n","    model = Sequential([\n","\n","        Conv1D(filters=64, kernel_size=5, activation='relu', padding='same', input_shape=input_shape),\n","        BatchNormalization(),\n","        Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),\n","        BatchNormalization(),\n","        MaxPooling1D(pool_size=2),\n","        Dropout(0.2),\n","\n","        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n","        BatchNormalization(),\n","        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n","        BatchNormalization(),\n","        MaxPooling1D(pool_size=2),\n","        Dropout(0.2),\n","\n","        GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n","        GRU(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n","\n","        Dense(128, activation='relu'),\n","        BatchNormalization(),\n","        Dropout(0.3),\n","\n","        Dense(64, activation='relu'),\n","        BatchNormalization(),\n","        Dropout(0.2),\n","\n","        Dense(num_classes, activation='softmax')\n","    ])\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n","        metrics=['accuracy']\n","    )\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"cnn_bilstm_model","metadata":{"id":"cnn_bilstm_model"},"outputs":[],"source":["def build_cnn_bidirectional_lstm_model(input_shape, num_classes):\n","\n","    model = Sequential([\n","\n","        Conv1D(filters=64, kernel_size=5, activation='relu', padding='same', input_shape=input_shape),\n","        BatchNormalization(),\n","        Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),\n","        BatchNormalization(),\n","        MaxPooling1D(pool_size=2),\n","        Dropout(0.2),\n","\n","        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n","        BatchNormalization(),\n","        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n","        BatchNormalization(),\n","        MaxPooling1D(pool_size=2),\n","        Dropout(0.2),\n","\n","        Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n","        Bidirectional(LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)),\n","\n","        Dense(128, activation='relu'),\n","        BatchNormalization(),\n","        Dropout(0.3),\n","\n","        Dense(64, activation='relu'),\n","        BatchNormalization(),\n","        Dropout(0.2),\n","\n","        Dense(num_classes, activation='softmax')\n","    ])\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n","        metrics=['accuracy']\n","    )\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"cnn_attention_lstm_model","metadata":{"id":"cnn_attention_lstm_model"},"outputs":[],"source":["def build_cnn_attention_lstm_model(input_shape, num_classes):\n","\n","    inputs = Input(shape=input_shape)\n","\n","    x = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs)\n","    x = BatchNormalization()(x)\n","    x = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = MaxPooling1D(pool_size=2)(x)\n","    x = Dropout(0.2)(x)\n","\n","    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = MaxPooling1D(pool_size=2)(x)\n","    x = Dropout(0.2)(x)\n","\n","    lstm_out = LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(x)\n","\n","    attention = MultiHeadAttention(num_heads=4, key_dim=32)(lstm_out, lstm_out)\n","    attention = LayerNormalization()(attention + lstm_out)  # Skip connection\n","\n","    avg_pool = GlobalAveragePooling1D()(attention)\n","    max_pool = GlobalMaxPooling1D()(attention)\n","    x = Concatenate()([avg_pool, max_pool])\n","\n","    x = Dense(128, activation='relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.3)(x)\n","\n","    x = Dense(64, activation='relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.2)(x)\n","\n","    outputs = Dense(num_classes, activation='softmax')(x)\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n","        metrics=['accuracy']\n","    )\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"model_selector","metadata":{"id":"model_selector"},"outputs":[],"source":["def build_model(model_type, input_shape, num_classes):\n","\n","    if model_type == 'cnn_lstm':\n","        model = build_cnn_lstm_model(input_shape, num_classes)\n","    elif model_type == 'cnn_gru':\n","        model = build_cnn_gru_model(input_shape, num_classes)\n","    elif model_type == 'cnn_bilstm':\n","        model = build_cnn_bidirectional_lstm_model(input_shape, num_classes)\n","    elif model_type == 'cnn_attention_lstm':\n","        model = build_cnn_attention_lstm_model(input_shape, num_classes)\n","    else:\n","        raise ValueError(f\"Unknown model type: {model_type}\")\n","\n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"evaluation","metadata":{"id":"evaluation"},"outputs":[],"source":["def evaluate_per_class_accuracy(y_true, y_pred, activity_names):\n","    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n","\n","\n","    print(\"PER-CLASS ACCURACY\")\n","\n","\n","    total_pass = 0\n","    class_accuracies = []\n","\n","    for i, activity in enumerate(activity_names):\n","        if cm[i, :].sum() > 0:\n","            acc = cm[i, i] / cm[i, :].sum()\n","            class_accuracies.append(acc)\n","            status = \"good\" if acc >= 0.75 else \"not good\"\n","            if acc >= 0.75:\n","                total_pass += 1\n","            print(f\"{activity:25s}: {acc:.4f} ({acc*100:.2f}%) {status}\")\n","        else:\n","            class_accuracies.append(0.0)\n","            print(f\"{activity:25s}: No samples in test set\")\n","\n","\n","    print(f\"{'Mean Class Accuracy':25s}: {np.mean(class_accuracies):.4f} ({np.mean(class_accuracies)*100:.2f}%)\")\n","    print(f\"Classes passing: {total_pass}/{len(activity_names)}\")\n","\n","\n","    return class_accuracies"]},{"cell_type":"markdown","id":"data_loading_section","metadata":{"id":"data_loading_section"},"source":["## Data Loading and Preprocessing"]},{"cell_type":"code","execution_count":null,"id":"load_data","metadata":{"id":"load_data"},"outputs":[],"source":["\n","print(f\"Model Type: {MODEL_TYPE}\")\n","\n","\n","# Defining activities\n","activities = {\n","    'breathingNormally': 0,\n","    'coughing': 1,\n","    'hyperventilation': 2,\n","    'other': 3\n","}\n","\n","activity_names = list(activities.keys())\n","\n","print(f\"\\nFound {len(activities)} social signal activities:\")\n","for activity, label in activities.items():\n","    print(f\"  [{label}] {activity}\")"]},{"cell_type":"code","execution_count":null,"id":"process_activities","metadata":{"id":"process_activities"},"outputs":[],"source":["\n","print(f\"Window size: {window_size}, Step size: {step_size}\")\n","\n","train_test_data = {}\n","\n","for activity, label in activities.items():\n","    print(f\"\\n  Processing {activity}...\")\n","    train_test_data[activity] = {}\n","\n","    (train_test_data[activity]['train_windows'],\n","     train_test_data[activity]['train_labels'],\n","     train_test_data[activity]['test_windows'],\n","     train_test_data[activity]['test_labels']) = process_activity(\n","        activity, label, your_dataset_path,\n","        window_size=window_size,\n","        step_size=step_size,\n","        test_size=0.2\n","    )\n","\n","    train_count = len(train_test_data[activity]['train_windows'])\n","    test_count = len(train_test_data[activity]['test_windows'])\n","    total_count = train_count + test_count\n","\n","    print(f\"    Total windows: {total_count} (Train: {train_count}, Test: {test_count})\")"]},{"cell_type":"code","execution_count":null,"id":"combine_and_filter","metadata":{"id":"combine_and_filter"},"outputs":[],"source":["#combining and filtering data\n","\n","X_train_all, y_train_all = combine_data(train_test_data, 'train')\n","X_test_all, y_test_all = combine_data(train_test_data, 'test')\n","\n","print(f\"Total windows - Train: {len(X_train_all)}, Test: {len(X_test_all)}\")\n","#filtering for static activities\n","X_train, y_train, train_static_indices = filter_static_windows(X_train_all, y_train_all)\n","X_test, y_test, test_static_indices = filter_static_windows(X_test_all, y_test_all)\n","\n","print(f\"Static windows - Train: {len(X_train)}, Test: {len(X_test)}\")\n","\n","print(\"\\nClass distribution after filtering:\")\n","for i, activity in enumerate(activity_names):\n","    train_count = np.sum(y_train == i)\n","    test_count = np.sum(y_test == i)\n","    print(f\"  {activity:20s}: Train={train_count:6d}, Test={test_count:5d}\")\n","\n","#normalizing the data\n","scaler = StandardScaler()\n","X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n","X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n","\n","X_train_normalized = scaler.fit_transform(X_train_reshaped)\n","X_test_normalized = scaler.transform(X_test_reshaped)\n","\n","X_train = X_train_normalized.reshape(X_train.shape)\n","X_test = X_test_normalized.reshape(X_test.shape)\n"]},{"cell_type":"code","execution_count":null,"id":"encode_labels","metadata":{"id":"encode_labels"},"outputs":[],"source":["#one hot encoding labels\n","\n","encoder = OneHotEncoder(sparse_output=False)\n","y_train_one_hot = encoder.fit_transform(y_train.reshape(-1, 1))\n","y_test_one_hot = encoder.transform(y_test.reshape(-1, 1))\n","\n","print(f\"Training samples per class:\")\n","for i, activity in enumerate(activity_names):\n","    count = np.sum(y_train == i)\n","    print(f\"  {activity}: {count}\")"]},{"cell_type":"markdown","id":"training_section","metadata":{"id":"training_section"},"source":["## Model Building and Training"]},{"cell_type":"code","execution_count":null,"id":"build_and_train","metadata":{"id":"build_and_train"},"outputs":[],"source":["\n","\n","# Building model\n","input_shape = (window_size, 3)\n","num_classes = len(activities)\n","\n","model = build_model(MODEL_TYPE, input_shape, num_classes)\n","\n","y_train_classes = np.argmax(y_train_one_hot, axis=1)\n","unique, counts = np.unique(y_train_classes, return_counts=True)\n","total_samples = len(y_train_classes)\n","\n","class_weight_dict = {}\n","for i in range(len(unique)):\n","    base_weight = total_samples / (len(unique) * counts[i])\n","\n","    if i == 0:\n","        class_weight_dict[i] = base_weight * 2.2\n","    elif i == 1:\n","        class_weight_dict[i] = base_weight * 3.8\n","    elif i == 2:\n","        class_weight_dict[i] = base_weight * 4.5\n","    elif i == 3:\n","        class_weight_dict[i] = base_weight * 1.3\n","\n","print(f\"\\nOptimized class weights: {class_weight_dict}\")\n","\n","early_stop = EarlyStopping(\n","    monitor='val_accuracy',\n","    patience=15,\n","    restore_best_weights=True,\n","    verbose=1,\n","    mode='max'\n",")\n","\n","reduce_lr = ReduceLROnPlateau(\n","    monitor='val_loss',\n","    factor=0.5,\n","    patience=5,\n","    min_lr=0.00001,\n","    verbose=1\n",")\n","\n","checkpoint = ModelCheckpoint(\n","    f'best_{MODEL_TYPE}_model.keras',\n","    monitor='val_accuracy',\n","    save_best_only=True,\n","    mode='max',\n","    verbose=1\n",")\n","\n","#training the model\n","#check epochs and batch size\n","print(f\"epochs: {epochs}, Batch size: {batch_size}\")\n","\n","history = model.fit(\n","    X_train, y_train_one_hot,\n","    epochs=epochs,\n","    batch_size=batch_size,\n","    validation_data=(X_test, y_test_one_hot),\n","    class_weight=class_weight_dict,\n","    callbacks=[early_stop, reduce_lr, checkpoint],\n","    verbose=1\n",")\n","\n"]},{"cell_type":"markdown","id":"evaluation_section","metadata":{"id":"evaluation_section"},"source":["## Model Evaluation"]},{"cell_type":"code","execution_count":null,"id":"evaluate_model","metadata":{"id":"evaluate_model"},"outputs":[],"source":["#evaluating the model\n","\n","y_pred_probs = model.predict(X_test)\n","y_pred = np.argmax(y_pred_probs, axis=1)\n","y_true = np.argmax(y_test_one_hot, axis=1)\n","\n","overall_acc = accuracy_score(y_true, y_pred)\n","\n","print(f\"OVERALL ACCURACY: {overall_acc:.4f} ({overall_acc*100:.2f}%)\")\n","\n","\n","class_accuracies = evaluate_per_class_accuracy(y_true, y_pred, activity_names)\n","\n","\n","print(\"CLASSIFICATION REPORT\")\n","\n","print(classification_report(y_true, y_pred, target_names=activity_names, digits=4))"]},{"cell_type":"code","execution_count":null,"id":"f86f46a6-d41a-402e-bc4e-304c606103b3","metadata":{"id":"f86f46a6-d41a-402e-bc4e-304c606103b3"},"outputs":[],"source":["\n","\n","report = classification_report(y_true, y_pred,\n","                              target_names=activity_names,\n","                              digits=4,\n","                              output_dict=True)\n","\n","classes = activity_names\n","precision_scores = [report[activity]['precision'] for activity in activity_names]\n","recall_scores = [report[activity]['recall'] for activity in activity_names]\n","f1_scores = [report[activity]['f1-score'] for activity in activity_names]\n","\n","x = np.arange(len(classes))\n","width = 0.25\n","\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","bars1 = ax.bar(x - width, precision_scores, width, label='Precision', color='#5B9BD5')\n","bars2 = ax.bar(x, recall_scores, width, label='Recall', color='#FF9F40')\n","bars3 = ax.bar(x + width, f1_scores, width, label='F1-Score', color='#70AD47')\n","\n","for bars in [bars1, bars2, bars3]:\n","    for bar in bars:\n","        height = bar.get_height()\n","        ax.text(bar.get_x() + bar.get_width()/2., height,\n","                f'{height:.2f}',\n","                ha='center', va='bottom', fontsize=10, fontweight='bold')\n","\n","ax.set_xlabel('Activity Class', fontsize=12, fontweight='bold')\n","ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n","ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n","ax.set_xticks(x)\n","ax.set_xticklabels(classes, rotation=0, ha='center')\n","ax.legend(loc='upper right', fontsize=11)\n","ax.set_ylim(0, 1.0)\n","ax.grid(axis='y', alpha=0.3, linestyle='--')\n","\n","ax.axhline(y=0.75, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='75% Threshold')\n","\n","plt.tight_layout()\n","plt.savefig(f'per_class_metrics_{MODEL_TYPE}.png', dpi=300, bbox_inches='tight')\n","plt.show()\n","\n","print(f\"\\nPer-class metrics bar graph saved as: per_class_metrics_{MODEL_TYPE}.png\")"]},{"cell_type":"code","execution_count":null,"id":"plot_confusion_matrix","metadata":{"id":"plot_confusion_matrix"},"outputs":[],"source":["cm = confusion_matrix(y_true, y_pred)\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=activity_names, yticklabels=activity_names)\n","plt.title(f'Confusion Matrix - {MODEL_TYPE.upper()}', fontsize=14, fontweight='bold')\n","plt.ylabel('True Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.tight_layout()\n","plt.savefig(f'confusion_matrix_{MODEL_TYPE}.png', dpi=300, bbox_inches='tight')\n","plt.show()\n","\n","cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n","            xticklabels=activity_names, yticklabels=activity_names)\n","plt.title(f'Normalized Confusion Matrix - {MODEL_TYPE.upper()}', fontsize=14, fontweight='bold')\n","plt.ylabel('True Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.tight_layout()\n","plt.savefig(f'confusion_matrix_normalized_{MODEL_TYPE}.png', dpi=300, bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"plot_training_history","metadata":{"id":"plot_training_history"},"outputs":[],"source":["fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","\n","#accuracy plot\n","ax1.plot(history.history['accuracy'], label='Train Accuracy')\n","ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","ax1.set_title(f'Model Accuracy - {MODEL_TYPE.upper()}', fontweight='bold')\n","ax1.set_xlabel('Epoch')\n","ax1.set_ylabel('Accuracy')\n","ax1.legend()\n","ax1.grid(True, alpha=0.3)\n","\n","#loss plot\n","ax2.plot(history.history['loss'], label='Train Loss')\n","ax2.plot(history.history['val_loss'], label='Validation Loss')\n","ax2.set_title(f'Model Loss - {MODEL_TYPE.upper()}', fontweight='bold')\n","ax2.set_xlabel('Epoch')\n","ax2.set_ylabel('Loss')\n","ax2.legend()\n","ax2.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(f'training_history_{MODEL_TYPE}.png', dpi=300, bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"save_model","metadata":{"id":"save_model"},"outputs":[],"source":["model.save(f'final_{MODEL_TYPE}_model.keras')\n","print(f\"\\nModel saved as: final_{MODEL_TYPE}_model.keras\")\n","\n","scaler_params = {\n","    'mean': scaler.mean_.tolist(),\n","    'scale': scaler.scale_.tolist()\n","}\n","with open(f'scaler_params_{MODEL_TYPE}.json', 'w') as f:\n","    json.dump(scaler_params, f)\n","print(f\"Scaler parameters saved as: scaler_params_{MODEL_TYPE}.json\")\n","\n","activity_mapping = {v: k for k, v in activities.items()}\n","with open(f'activity_mapping_{MODEL_TYPE}.json', 'w') as f:\n","    json.dump(activity_mapping, f)\n","print(f\"Activity mapping saved as: activity_mapping_{MODEL_TYPE}.json\")"]},{"cell_type":"markdown","id":"summary","metadata":{"id":"summary"},"source":["## Final Summary"]},{"cell_type":"code","execution_count":null,"id":"final_summary","metadata":{"id":"final_summary"},"outputs":[],"source":["\n","print(f\"Final Results Summary - {MODEL_TYPE.upper()}\")\n","\n","print(f\"Overall Accuracy: {overall_acc*100:.2f}%\")\n","print(f\"Mean Class Accuracy: {np.mean(class_accuracies)*100:.2f}%\")\n","print(f\"Total Test Samples: {len(X_test)}\")\n","print(f\"\\nPer-Class Performance:\")\n","for i, activity in enumerate(activity_names):\n","    mask = y_true == i\n","    if np.sum(mask) > 0:\n","        class_acc = class_accuracies[i]\n","        status = \"good\" if class_acc >= 0.75 else \"not good\"\n","        print(f\"  {status} {activity:20s}: {class_acc*100:.2f}%\")\n","\n","classes_passing = sum(1 for acc in class_accuracies if acc >= 0.75)\n","print(f\"\\nClasses achieving >75% accuracy: {classes_passing}/{len(activity_names)}\")"]},{"cell_type":"code","execution_count":null,"id":"8ecf238f-adea-4484-b3ff-ca5df49a970a","metadata":{"id":"8ecf238f-adea-4484-b3ff-ca5df49a970a"},"outputs":[],"source":["#converting to tflite format\n","\n","model_path = f'best_{MODEL_TYPE}_model.keras'\n","print(f\"\\nLoading trained model: {model_path}\")\n","\n","if not os.path.exists(model_path):\n","    raise FileNotFoundError(f\"Trained model not found: {model_path}\")\n","\n","model = tf.keras.models.load_model(model_path)\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","\n","converter.target_spec.supported_ops = [\n","    tf.lite.OpsSet.TFLITE_BUILTINS,\n","    tf.lite.OpsSet.SELECT_TF_OPS\n","]\n","converter._experimental_lower_tensor_list_ops = False\n","\n","tflite_model = converter.convert()\n","\n","tflite_filename = f'{MODEL_TYPE}_model.tflite'\n","with open(tflite_filename, 'wb') as f:\n","    f.write(tflite_model)\n","\n","print(f\"TFLite model saved: {tflite_filename}\")\n","print(f\"  Size: {os.path.getsize(tflite_filename) / 1024:.2f} KB\")\n","\n","#saving the metadata\n","\n","metadata = {\n","    'model_type': MODEL_TYPE,\n","    'window_size': window_size,\n","    'step_size': step_size,\n","    'activity_mapping': activities,\n","    'scaler_mean': scaler.mean_.tolist(),\n","    'scaler_scale': scaler.scale_.tolist(),\n","    'training_date': pd.Timestamp.now().isoformat(),\n","    'includes_tf_ops': True,\n","    'model_architecture': 'hybrid_cnn_rnn'\n","}\n","\n","metadata_filename = f'{MODEL_TYPE}_metadata.json'\n","with open(metadata_filename, 'w') as f:\n","    json.dump(metadata, f, indent=2)\n","\n","print(f\"Metadata saved: {metadata_filename}\")\n","\n","\n","try:\n","    interpreter = tf.lite.Interpreter(model_path=tflite_filename)\n","    interpreter.allocate_tensors()\n","\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","\n","    print(f\"  Input shape:  {input_details[0]['shape']}\")\n","    print(f\"  Output shape: {output_details[0]['shape']}\")\n","\n","    # Test with a sample input\n","    test_input = np.random.randn(1, window_size, 3).astype(np.float32)\n","    interpreter.set_tensor(input_details[0]['index'], test_input)\n","    interpreter.invoke()\n","    test_output = interpreter.get_tensor(output_details[0]['index'])\n","\n","    print(f\"  Test prediction shape: {test_output.shape}\")\n","    print(f\"  Test prediction: {test_output[0]}\")\n","\n","except RuntimeError as e:\n","    print(f\"Error\")\n","\n","print(f\"\\nGenerated files:\")\n","print(f\"  1.  {tflite_filename}\")\n","print(f\"  2.  {metadata_filename}\")"]},{"cell_type":"code","execution_count":null,"id":"7febda97-5fd0-409f-9c03-c6d52928b671","metadata":{"id":"7febda97-5fd0-409f-9c03-c6d52928b671"},"outputs":[],"source":["#test model on new dataset\n","\n","possible_zip_locations = [\n","    os.path.join(notebook_dir, \"data\", \"RESpeckData_2526_fixed.zip\"),\n","    os.path.join(notebook_dir, \"RESpeckData_2526_fixed.zip\"),\n","]\n","\n","zip_path = None\n","for location in possible_zip_locations:\n","    if os.path.exists(location):\n","        zip_path = location\n","        break\n","\n","if zip_path is None:\n","    raise FileNotFoundError(\"error\")\n","\n","extract_path = os.path.join(notebook_dir, \"temp_test_data\")\n","\n","if os.path.exists(extract_path):\n","    shutil.rmtree(extract_path)\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)\n","\n","print(f\"✓ Dataset extracted to: {extract_path}\")\n","\n","data_path = os.path.join(extract_path, \"RESpeckData_2526_fixed\", \"social_signal\")\n","print(f\"  Looking for data in: {data_path}\")\n","\n","keras_files = glob.glob('*.keras')\n","if keras_files:\n","    model_path = keras_files[0]\n","    print(f\"  Found model: {model_path}\")\n","else:\n","    model_path = f'best_{MODEL_TYPE}_model.keras'\n","    if not os.path.exists(model_path):\n","        raise FileNotFoundError(f\"Model not found: {model_path}\")\n","\n","model = tf.keras.models.load_model(model_path)\n","print(f\"Model loaded\")\n","print(f\"  Input shape:  {model.input_shape}\")\n","print(f\"  Output shape: {model.output_shape}\")\n","\n","\n","test_window_size = window_size\n","test_step_size = step_size\n","test_scaler_mean = scaler.mean_\n","test_scaler_scale = scaler.scale_\n","\n","test_activities = [str(act) for act in activities]\n","\n","print(f\"Configuration loaded:\")\n","print(f\"  Window size: {test_window_size}\")\n","print(f\"  Step size: {test_step_size}\")\n","print(f\"  Training activities: {test_activities}\")\n","\n","folder_to_training_label = {\n","    'breathing': 'breathingNormally',\n","    'Breathing': 'breathingNormally',\n","    'coughing': 'coughing',\n","    'Coughing': 'coughing',\n","    'hyperventilate': 'hyperventilation',\n","    'Hyperventilate': 'hyperventilation',\n","    'other': 'other',\n","    'Other': 'other'\n","}\n","\n","if os.path.exists(data_path):\n","    available_folders = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))]\n","    print(f\"  Available folders: {available_folders}\")\n","else:\n","    raise FileNotFoundError(f\"error in: {data_path}\")\n","\n","folder_to_label = {\n","    folder: label\n","    for folder, label in folder_to_training_label.items()\n","    if folder in available_folders\n","}\n","\n","if not folder_to_label:\n","    print(f\"   Available: {available_folders}\")\n","    print(f\"   Expected: {list(folder_to_training_label.keys())}\")\n","    raise ValueError(\"No matching folders in test dataset\")\n","\n","for folder, label in folder_to_label.items():\n","    print(f\"  {folder:20s} → {label}\")\n","\n","test_labels = list(set(folder_to_label.values()))\n","print(f\"\\nSummary:\")\n","print(f\"   Test dataset has {len(test_labels)} classes: {test_labels}\")\n","print(f\"   Model trained on {len(test_activities)} classes: {test_activities}\")\n","missing_classes = set(test_activities) - set(test_labels)\n","if missing_classes:\n","    print(f\"Missing from test: {missing_classes}\")\n","\n","\n","def load_test_data(data_path, folder_to_label, window_size, step_size):\n","    all_windows = []\n","    all_labels = []\n","\n","    for folder_name, training_label in folder_to_label.items():\n","        folder_path = os.path.join(data_path, folder_name)\n","\n","        if not os.path.exists(folder_path):\n","            print(f\"Folder '{folder_name}' not found\")\n","            continue\n","\n","        csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n","        print(f\"  Loading {folder_name:20s} → {training_label:20s}: {len(csv_files):3d} files\", end='')\n","\n","        windows_loaded = 0\n","        for csv_file in csv_files:\n","            try:\n","\n","                df = pd.read_csv(csv_file)\n","\n","\n","                if not all(col in df.columns for col in ['accelX', 'accelY', 'accelZ']):\n","                    continue\n","\n","\n","                accel_data = df[['accelX', 'accelY', 'accelZ']].values\n","\n","\n","                num_windows = (len(accel_data) - window_size) // step_size + 1\n","\n","                for i in range(num_windows):\n","                    start_idx = i * step_size\n","                    end_idx = start_idx + window_size\n","\n","                    if end_idx <= len(accel_data):\n","                        window = accel_data[start_idx:end_idx]\n","                        all_windows.append(window)\n","                        all_labels.append(training_label)\n","                        windows_loaded += 1\n","            except Exception as e:\n","                continue\n","\n","        print(f\" → {windows_loaded} windows\")\n","\n","    return np.array(all_windows), np.array(all_labels)\n","\n","X_test, y_test = load_test_data(data_path, folder_to_label, test_window_size, test_step_size)\n","\n","print(f\"\\nTest data loaded:\")\n","print(f\"  Total windows: {len(X_test)}\")\n","\n","if len(X_test) == 0:\n","    raise ValueError(\"No test data loaded.\")\n","\n","unique_labels, counts = np.unique(y_test, return_counts=True)\n","print(f\"\\n  Class distribution (training labels):\")\n","for label, count in zip(unique_labels, counts):\n","    print(f\"    {label:20s}: {count:5d} windows\")\n","\n","#normalising data\n","\n","X_test_reshaped = X_test.reshape(-1, 3)\n","\n","X_test_normalized = (X_test_reshaped - test_scaler_mean) / (test_scaler_scale + 1e-8)\n","X_test_normalized = X_test_normalized.reshape(-1, test_window_size, 3)\n","\n","#making predictions\n","predictions_proba = model.predict(X_test_normalized, batch_size=128, verbose=0)\n","predictions = np.argmax(predictions_proba, axis=1)\n","\n","\n","label_to_idx = {str(label): idx for idx, label in enumerate(test_activities)}\n","\n","print(f\"  Training label to index mapping:\")\n","for label, idx in label_to_idx.items():\n","    print(f\"    '{label}' → {idx}\")\n","\n","y_test_indices = []\n","unmapped_count = 0\n","\n","for i, label in enumerate(y_test):\n","    label_str = str(label)\n","    if label_str in label_to_idx:\n","        y_test_indices.append(label_to_idx[label_str])\n","    else:\n","        print(f\"erroe\")\n","        unmapped_count += 1\n","\n","y_test_indices = np.array(y_test_indices)\n","\n","if unmapped_count > 0:\n","    print(f\" {unmapped_count} samples had unmapped labels\")\n","else:\n","    print(f\"All {len(y_test_indices)} labels mapped\")\n","\n","test_accuracy = accuracy_score(y_test_indices, predictions)\n","print(f\"overall accuracy: {test_accuracy*100:.2f}%\")\n","\n","activity_names = test_activities\n","class_accuracies = []\n","\n","print(f\"\\nper class accuracy:\")\n","for i, activity in enumerate(activity_names):\n","    mask = y_test_indices == i\n","    if np.sum(mask) > 0:\n","        class_acc = accuracy_score(y_test_indices[mask], predictions[mask])\n","        class_accuracies.append(class_acc)\n","        status = \"PASS\" if class_acc >= 0.75 else \" FAIL\"\n","        print(f\"{status} {activity:20s}: {class_acc*100:6.2f}% ({np.sum(mask):5d} samples)\")\n","    else:\n","        print(f\"  - {activity:20s}: No samples in test set\")\n","        class_accuracies.append(0.0)\n","\n","mean_class_acc = np.mean([acc for acc in class_accuracies if acc > 0])\n","classes_with_samples = sum(1 for acc in class_accuracies if acc > 0)\n","classes_passing = sum(1 for acc in class_accuracies if acc >= 0.75)\n","\n","print(f\"\\n{'Mean Class Accuracy':23s}: {mean_class_acc*100:6.2f}%\")\n","print(f\"Classes tested: {classes_with_samples}/{len(activity_names)}\")\n","print(f\"Classes ≥75%: {classes_passing}/{classes_with_samples}\")\n","\n","print(f\"\\nclassification report:\")\n","print(classification_report(y_test_indices, predictions,\n","                           target_names=activity_names,\n","                           digits=4,\n","                           zero_division=0))\n","\n","#confusion matrix calculations\n","cm = confusion_matrix(y_test_indices, predictions)\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=activity_names, yticklabels=activity_names)\n","plt.title(f'Confusion Matrix - {MODEL_TYPE.upper()}', fontsize=14, fontweight='bold')\n","plt.ylabel('True Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.tight_layout()\n","plt.savefig(f'test_confusion_matrix_{MODEL_TYPE}.png', dpi=300, bbox_inches='tight')\n","plt.show()\n","print(f\"Saved: test_confusion_matrix_{MODEL_TYPE}.png\")\n","\n","cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)\n","\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn',\n","            xticklabels=activity_names, yticklabels=activity_names,\n","            vmin=0, vmax=1)\n","plt.title(f'Normalized Confusion Matrix - {MODEL_TYPE.upper()}', fontsize=14, fontweight='bold')\n","plt.ylabel('True Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.tight_layout()\n","plt.savefig(f'test_confusion_matrix_normalized_{MODEL_TYPE}.png', dpi=300, bbox_inches='tight')\n","plt.show()\n","print(f\"✓ Saved: test_confusion_matrix_normalized_{MODEL_TYPE}.png\")\n","\n","\n","results = {\n","    'model_type': MODEL_TYPE,\n","    'model_path': model_path,\n","    'test_dataset': zip_path,\n","    'label_mapping': folder_to_label,\n","    'overall_accuracy': float(test_accuracy),\n","    'mean_class_accuracy': float(mean_class_acc),\n","    'classes_tested': int(classes_with_samples),\n","    'classes_passing': int(classes_passing),\n","    'per_class_accuracy': {\n","        activity: float(class_accuracies[i])\n","        for i, activity in enumerate(activity_names)\n","        if class_accuracies[i] > 0\n","    },\n","    'test_samples': int(len(X_test))\n","}\n","\n","with open(f'test_results_{MODEL_TYPE}_new_dataset.json', 'w') as f:\n","    json.dump(results, f, indent=2)\n","\n","print(f\"Results saved to: test_results_{MODEL_TYPE}_new_dataset.json\")\n","\n","#printing the summary:\n","print(\"summary\")\n","\n","print(f\"Model:                {MODEL_TYPE.upper()}\")\n","print(f\"Overall Accuracy:     {test_accuracy*100:.2f}%\")\n","print(f\"Mean Class Accuracy:  {mean_class_acc*100:.2f}%\")\n","print(f\"Total Test Samples:   {len(X_test)}\")\n","print(f\"Classes Tested:       {classes_with_samples}/{len(activity_names)}\")\n","print(f\"Classes ≥75%:         {classes_passing}/{classes_with_samples}\")\n","\n","print(f\"\\nLabel Mapping:\")\n","for test_label, train_label in folder_to_label.items():\n","    print(f\"  {test_label:20s} → {train_label}\")\n","\n","print(f\"\\nPer-Class Results:\")\n","\n","for i, activity in enumerate(activity_names):\n","    if class_accuracies[i] > 0:\n","        status = \"good\" if class_accuracies[i] >= 0.75 else \"not good\"\n","        print(f\"{status} {activity:20s}: {class_accuracies[i]*100:6.2f}%\")\n","    else:\n","        print(f\"  - {activity:20s}: Not in test set\")\n","\n","if mean_class_acc >= 0.75:\n","    print(\"\\ntarget met\")\n","else:\n","    print(f\"\\n Mean accuracy: {mean_class_acc*100:.1f}%\")\n","    print(f\"   Classes passing: {classes_passing}/{classes_with_samples}\")"]},{"cell_type":"code","execution_count":null,"id":"c3242acb-dfbc-4683-aca9-ae783e582fd4","metadata":{"id":"c3242acb-dfbc-4683-aca9-ae783e582fd4"},"outputs":[],"source":["#k fold cross validation\n","activities = {\n","    'breathingNormally': 0,\n","    'coughing': 1,\n","    'hyperventilation': 2,\n","    'other': 3\n","}\n","activity_names = list(activities.keys())\n","\n","subject_data = {}\n","subject_labels = {}\n","\n","for activity_name, label in activities.items():\n","    activity_folder = os.path.join(your_dataset_path, activity_name)\n","\n","    if not os.path.exists(activity_folder):\n","        print(f\"Warning: Folder {activity_folder} not found\")\n","        continue\n","\n","    files = load_files_from_folder(activity_folder)\n","    print(f\"{activity_name}: Found {len(files)} subject files\")\n","\n","    for file_idx, file_path in enumerate(files):\n","        subject_id = f\"{activity_name}_S{file_idx}\"\n","\n","        df = pd.read_csv(file_path)\n","        accel_data = df[['accelX', 'accelY']].values\n","\n","\n","        windows = []\n","        for i in range(0, len(accel_data) - window_size + 1, step_size):\n","            window = accel_data[i:i + window_size]\n","            if len(window) == window_size:\n","                windows.append(window)\n","\n","        if len(windows) > 0:\n","            if subject_id not in subject_data:\n","                subject_data[subject_id] = []\n","                subject_labels[subject_id] = []\n","\n","            subject_data[subject_id].extend(windows)\n","            subject_labels[subject_id].extend([label] * len(windows))\n","\n","\n","for subject_id in subject_data.keys():\n","    subject_data[subject_id] = np.array(subject_data[subject_id])\n","    subject_labels[subject_id] = np.array(subject_labels[subject_id])\n","\n","subject_ids = np.array(list(subject_data.keys()))\n","n_subjects = len(subject_ids)\n","\n","print(f\"\\nTotal subjects: {n_subjects}\")\n","\n","N_FOLDS = 5\n","kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n","\n","fold_results = []\n","all_predictions = []\n","all_true_labels = []\n","class_accuracies_per_fold = []\n","\n","for fold_idx, (train_idx, test_idx) in enumerate(kf.split(subject_ids)):\n","    print(f\"FOLD {fold_idx + 1}/{N_FOLDS}\")\n","\n","    train_subjects = subject_ids[train_idx]\n","    test_subjects = subject_ids[test_idx]\n","\n","    print(f\"Train subjects: {len(train_subjects)}, Test subjects: {len(test_subjects)}\")\n","\n","    X_train_fold = []\n","    y_train_fold = []\n","    for subj in train_subjects:\n","        X_train_fold.append(subject_data[subj])\n","        y_train_fold.append(subject_labels[subj])\n","\n","    X_train_fold = np.vstack(X_train_fold)\n","    y_train_fold = np.concatenate(y_train_fold)\n","\n","    X_test_fold = []\n","    y_test_fold = []\n","    for subj in test_subjects:\n","        X_test_fold.append(subject_data[subj])\n","        y_test_fold.append(subject_labels[subj])\n","\n","    X_test_fold = np.vstack(X_test_fold)\n","    y_test_fold = np.concatenate(y_test_fold)\n","\n","    print(f\"Train samples: {len(X_train_fold)}, Test samples: {len(X_test_fold)}\")\n","\n","    print(\"\\nClass distribution:\")\n","    for i, activity in enumerate(activity_names):\n","        train_count = np.sum(y_train_fold == i)\n","        test_count = np.sum(y_test_fold == i)\n","        print(f\"  {activity:20s}: Train={train_count:5d}, Test={test_count:4d}\")\n","\n","    scaler = StandardScaler()\n","    X_train_reshaped = X_train_fold.reshape(-1, X_train_fold.shape[-1])\n","    X_test_reshaped = X_test_fold.reshape(-1, X_test_fold.shape[-1])\n","\n","    X_train_normalized = scaler.fit_transform(X_train_reshaped).reshape(X_train_fold.shape)\n","    X_test_normalized = scaler.transform(X_test_reshaped).reshape(X_test_fold.shape)\n","\n","    y_train_one_hot = tf.keras.utils.to_categorical(y_train_fold, num_classes=len(activities))\n","    y_test_one_hot = tf.keras.utils.to_categorical(y_test_fold, num_classes=len(activities))\n","\n","    input_shape = (window_size, 2)\n","    model = build_model(MODEL_TYPE, input_shape, len(activities))\n","\n","    class_weights = compute_class_weight('balanced',\n","                                         classes=np.unique(y_train_fold),\n","                                         y=y_train_fold)\n","    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n","\n","    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n","\n","    #training the model\n","    history = model.fit(\n","        X_train_normalized, y_train_one_hot,\n","        validation_split=0.2,\n","        epochs=30,\n","        batch_size=batch_size,\n","        class_weight=class_weight_dict,\n","        callbacks=[early_stop],\n","        verbose=0\n","    )\n","\n","    print(f\"Training completed in {len(history.history['loss'])} epochs\")\n","\n","    y_pred_probs = model.predict(X_test_normalized, verbose=0)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","    y_true = y_test_fold\n","\n","    fold_acc = accuracy_score(y_true, y_pred)\n","    fold_f1 = f1_score(y_true, y_pred, average='weighted')\n","    fold_precision = precision_score(y_true, y_pred, average='weighted')\n","    fold_recall = recall_score(y_true, y_pred, average='weighted')\n","\n","    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n","    class_accs = []\n","    for i in range(len(activities)):\n","        if cm[i, :].sum() > 0:\n","            class_acc = cm[i, i] / cm[i, :].sum()\n","            class_accs.append(class_acc)\n","        else:\n","            class_accs.append(0.0)\n","\n","    class_accuracies_per_fold.append(class_accs)\n","\n","    print(f\"\\nFold {fold_idx + 1} Results:\")\n","    print(f\"  Overall Accuracy: {fold_acc:.4f} ({fold_acc*100:.2f}%)\")\n","    print(f\"  Precision:        {fold_precision:.4f}\")\n","    print(f\"  Recall:           {fold_recall:.4f}\")\n","    print(f\"  F1-Score:         {fold_f1:.4f}\")\n","    print(f\"\\n  Per-class Accuracy:\")\n","    for i, activity in enumerate(activity_names):\n","        status = \"good\" if class_accs[i] >= 0.75 else \"not good\"\n","        print(f\"    {status} {activity:20s}: {class_accs[i]:.4f} ({class_accs[i]*100:.2f}%)\")\n","\n","    fold_results.append({\n","        'fold': fold_idx + 1,\n","        'n_train_subjects': len(train_subjects),\n","        'n_test_subjects': len(test_subjects),\n","        'accuracy': fold_acc,\n","        'precision': fold_precision,\n","        'recall': fold_recall,\n","        'f1_score': fold_f1,\n","        'class_accuracies': class_accs\n","    })\n","\n","    all_predictions.extend(y_pred)\n","    all_true_labels.extend(y_true)\n","\n","\n","\n","print(\"CV summary\")\n","\n","\n","overall_cv_accuracy = accuracy_score(all_true_labels, all_predictions)\n","overall_cv_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n","overall_cv_precision = precision_score(all_true_labels, all_predictions, average='weighted')\n","overall_cv_recall = recall_score(all_true_labels, all_predictions, average='weighted')\n","\n","print(f\"\\nOverall Cross-Validation Metrics (Aggregated across all folds):\")\n","print(f\"  Accuracy:  {overall_cv_accuracy:.4f} ({overall_cv_accuracy*100:.2f}%)\")\n","print(f\"  Precision: {overall_cv_precision:.4f} ({overall_cv_precision*100:.2f}%)\")\n","print(f\"  Recall:    {overall_cv_recall:.4f} ({overall_cv_recall*100:.2f}%)\")\n","print(f\"  F1-Score:  {overall_cv_f1:.4f} ({overall_cv_f1*100:.2f}%)\")\n","\n","fold_accuracies = [r['accuracy'] for r in fold_results]\n","fold_f1s = [r['f1_score'] for r in fold_results]\n","\n","print(f\"\\nPer-Fold Statistics:\")\n","print(f\"  Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n","print(f\"  F1-Score: {np.mean(fold_f1s):.4f} ± {np.std(fold_f1s):.4f}\")\n","\n","class_accuracies_array = np.array(class_accuracies_per_fold)\n","mean_class_accs = np.mean(class_accuracies_array, axis=0)\n","std_class_accs = np.std(class_accuracies_array, axis=0)\n","\n","\n","print(\"per class CV accuracy\")\n","\n","\n","print(f\"\\n{'Activity':<22} {'Mean Accuracy':<20} {'Status'}\")\n","\n","for i, activity in enumerate(activity_names):\n","    status = \"good\" if mean_class_accs[i] >= 0.75 else \"not good\"\n","    print(f\"{activity:<22} {mean_class_accs[i]:.4f} ± {std_class_accs[i]:.4f} \"\n","          f\"({mean_class_accs[i]*100:.2f}%)    {status}\")\n","\n","\n","print(\"Confusion Matrix (combined):\")\n","\n","\n","cm_overall = confusion_matrix(all_true_labels, all_predictions, labels=[0, 1, 2, 3])\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n","\n","sns.heatmap(cm_overall, annot=True, fmt='d', cmap='Blues', ax=ax1,\n","            xticklabels=activity_names, yticklabels=activity_names,\n","            cbar_kws={'label': 'Count'})\n","ax1.set_title(f'Confusion Matrix - Raw Counts\\n{MODEL_TYPE.upper()}', fontsize=12, pad=15)\n","ax1.set_ylabel('True Label')\n","ax1.set_xlabel('Predicted Label')\n","\n","cm_normalized = cm_overall.astype('float') / cm_overall.sum(axis=1)[:, np.newaxis]\n","sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=ax2,\n","            xticklabels=activity_names, yticklabels=activity_names,\n","            cbar_kws={'label': 'Percentage'})\n","ax2.set_title(f'Confusion Matrix - Normalized\\n{MODEL_TYPE.upper()}', fontsize=12, pad=15)\n","ax2.set_ylabel('True Label')\n","ax2.set_xlabel('Predicted Label')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","#Classification report\n","\n","print(\"classification report\")\n","\n","print(classification_report(all_true_labels, all_predictions,\n","                          target_names=activity_names, digits=4))\n","\n","\n","cv_results_summary = {\n","    'model_type': MODEL_TYPE,\n","    'n_folds': N_FOLDS,\n","    'n_subjects': n_subjects,\n","    'overall_metrics': {\n","        'accuracy': float(overall_cv_accuracy),\n","        'precision': float(overall_cv_precision),\n","        'recall': float(overall_cv_recall),\n","        'f1_score': float(overall_cv_f1)\n","    },\n","    'fold_statistics': {\n","        'mean_accuracy': float(np.mean(fold_accuracies)),\n","        'std_accuracy': float(np.std(fold_accuracies)),\n","        'mean_f1': float(np.mean(fold_f1s)),\n","        'std_f1': float(np.std(fold_f1s))\n","    },\n","    'per_class_accuracy': {\n","        activity_names[i]: {\n","            'mean': float(mean_class_accs[i]),\n","            'std': float(std_class_accs[i]),\n","            'pass_threshold': bool(mean_class_accs[i] >= 0.75)\n","        }\n","        for i in range(len(activity_names))\n","    },\n","    'fold_details': fold_results\n","}\n","\n","cv_results_file = f'cv_results_{MODEL_TYPE}_{N_FOLDS}fold.json'\n","with open(cv_results_file, 'w') as f:\n","    json.dump(cv_results_summary, f, indent=2)\n","\n","print(f\"\\nCross-validation results saved to: {cv_results_file}\")\n","\n","\n","\n","print(\"Summary table for Report \")\n","\n","\n","print(\"\\nTable: Cross-Validation Performance Metrics\")\n","\n","print(f\"{'Metric':<30} {'Value':<20} {'Status'}\")\n","\n","print(f\"{'Overall Accuracy':<30} {overall_cv_accuracy:.4f} ({overall_cv_accuracy*100:.2f}%)\")\n","print(f\"{'Overall F1-Score':<30} {overall_cv_f1:.4f}\")\n","print(f\"{'Number of Folds':<30} {N_FOLDS}\")\n","print(f\"{'Total Subjects':<30} {n_subjects}\")\n","\n","print(\"\\nTable: Per-Class Accuracy (Cross-Validation)\")\n","\n","print(f\"{'Activity Class':<30} {'Accuracy':<20} {'Status'}\")\n","\n","for i, activity in enumerate(activity_names):\n","    status = \"good\" if mean_class_accs[i] >= 0.75 else \"not good\"\n","    print(f\"{activity:<30} {mean_class_accs[i]:.4f} ({mean_class_accs[i]*100:.2f}%)    {status}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}